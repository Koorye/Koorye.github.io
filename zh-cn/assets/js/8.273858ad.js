(window.webpackJsonp=window.webpackJsonp||[]).push([[8],{209:function(r,t,e){"use strict";var o=e(72);e.n(o).a},223:function(r,t,e){"use strict";e.r(t);e(209);var o=e(0),n=Object(o.a)({},(function(){var r=this,t=r.$createElement,e=r._self._c||t;return e("ContentSlotsDistributor",{attrs:{"slot-key":r.$parent.slotKey}},[e("ProfileSection",{attrs:{frontmatter:r.$page.frontmatter}}),r._v(" "),e("p",[e("img",{attrs:{src:"https://img.shields.io/github/stars/Koorye?style=for-the-badge",alt:"GitHub User's stars"}}),r._v(" "),e("img",{attrs:{src:"https://img.shields.io/github/followers/Koorye?style=for-the-badge",alt:"GitHub User's followers"}})]),r._v(" "),e("h2",{attrs:{id:"自我介绍"}},[r._v("自我介绍")]),r._v(" "),e("p",[e("strong",[r._v("电子科技大学计算机科学与工程学院学术硕士在读")]),r._v("，专业为计算机科学与技术，GPA排名"),e("strong",[r._v("前1.3%")]),r._v("。研究方向包括：")]),r._v(" "),e("ul",[e("li",[e("strong",[r._v("视觉语言模型")]),r._v(" (VLMs) 的高效迁移学习")]),r._v(" "),e("li",[e("strong",[r._v("视觉语言动作模型")]),r._v(" (VLAs) 的训练和测试时适应")])]),r._v(" "),e("p",[r._v("已作为第一/共同第一作者发表2篇"),e("strong",[r._v("CVPR（CCF-A）"),e("strong",[r._v("论文，并获得")]),r._v("国家奖学金")]),r._v("和"),e("strong",[r._v("优秀毕业生")]),r._v("荣誉。")]),r._v(" "),e("h2",{attrs:{id:"新闻"}},[r._v("新闻")]),r._v(" "),e("p",[r._v("[2025.5.20] 🔥 我们的论文 "),e("strong",[r._v('"InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning"')]),r._v(" 已经发布！")]),r._v(" "),e("p",[r._v("[2025.5.19] 🔥 我们的论文 "),e("strong",[r._v('"Policy Contrastive Decoding for Robotic Foundation Models"')]),r._v(" 已经发布！")]),r._v(" "),e("h2",{attrs:{id:"教育经历"}},[r._v("教育经历")]),r._v(" "),e("p",[e("router-link",{attrs:{to:"/about/"}},[r._v("→ 完整列表")])],1),r._v(" "),e("p",[r._v("学术硕士在读, "),e("strong",[r._v("电子科技大学计算机科学与工程学院")]),r._v("，计算机科学与技术, 2023 - 至今")]),r._v(" "),e("p",[r._v("工学学士, "),e("strong",[r._v("电子科技大学软件与信息工程学院")]),r._v("，软件工程, 2019 - 2023")]),r._v(" "),e("h2",{attrs:{id:"发表论文"}},[r._v("发表论文")]),r._v(" "),e("p",[e("router-link",{attrs:{to:"/publications/"}},[r._v("→ 完整列表")])],1),r._v(" "),e("p",[e("em",[r._v("注意: (*代表相同贡献)")])]),r._v(" "),e("ProjectCard",{attrs:{image:"/pubs/pcd.png",hideBorder:"true"}},[e("p",[e("strong",[r._v("Policy Contrastive Decoding for Robotic Foundation Models")])]),r._v(" "),e("p",[e("strong",[r._v("Shihan Wu")]),r._v("*, Ji Zhang*, Xu Luo, Junlin Xie, Jingkuan Song, Heng Tao Shen, Lianli Gao")]),r._v(" "),e("p",[r._v("机器人学 · 视觉语言动作模型 · 对比解码")]),r._v(" "),e("p",[r._v("2025.5")]),r._v(" "),e("p",[e("a",{attrs:{href:"https://koorye.github.io/proj/PCD",target:"_blank",rel:"noopener noreferrer"}},[r._v("[项目主页]"),e("OutboundLink")],1),r._v(" "),e("a",{attrs:{href:"https://arxiv.org/pdf/2505.13255",target:"_blank",rel:"noopener noreferrer"}},[r._v("[PDF]"),e("OutboundLink")],1),r._v(" "),e("a",{attrs:{href:"https://arxiv.org/abs/2505.13255",target:"_blank",rel:"noopener noreferrer"}},[r._v("[arXiv]"),e("OutboundLink")],1),r._v(" "),e("a",{attrs:{href:"https://github.com/Koorye/PCD",target:"_blank",rel:"noopener noreferrer"}},[r._v("[代码]"),e("OutboundLink")],1)]),r._v(" "),e("p",[e("img",{attrs:{src:"https://img.shields.io/github/stars/Koorye/PCD?style=for-the-badge&labelColor=%23eeeeee",alt:"GitHub Repo stars"}})])]),r._v(" "),e("ProjectCard",{attrs:{image:"/pubs/inspire.png",hideBorder:"true"}},[e("p",[e("strong",[r._v("InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning")])]),r._v(" "),e("p",[r._v("Ji Zhang*, "),e("strong",[r._v("Shihan Wu")]),r._v("*, Xu Luo, Hao Wu, Lianli Gao, Heng Tao Shen, Jingkuan Song")]),r._v(" "),e("p",[r._v("机器人学 · 视觉语言动作模型 · 虚假相关性")]),r._v(" "),e("p",[r._v("2025.5")]),r._v(" "),e("p",[e("a",{attrs:{href:"https://koorye.github.io/proj/Inspire",target:"_blank",rel:"noopener noreferrer"}},[r._v("[项目主页]"),e("OutboundLink")],1),r._v(" "),e("a",{attrs:{href:"https://arxiv.org/pdf/2412.11509",target:"_blank",rel:"noopener noreferrer"}},[r._v("[PDF]"),e("OutboundLink")],1),r._v(" "),e("a",{attrs:{href:"https://arxiv.org/abs/2412.11509",target:"_blank",rel:"noopener noreferrer"}},[r._v("[arXiv]"),e("OutboundLink")],1),r._v(" "),e("a",{attrs:{href:"https://github.com/Koorye.SkipTuning",target:"_blank",rel:"noopener noreferrer"}},[r._v("[代码]"),e("OutboundLink")],1)]),r._v(" "),e("p",[e("img",{attrs:{src:"https://img.shields.io/github/stars/Koorye/InSpire?style=for-the-badge&labelColor=%23eeeeee",alt:"GitHub Repo stars"}})])]),r._v(" "),e("ProjectCard",{attrs:{image:"/pubs/skiptuning.png",hideBorder:"true"}},[e("p",[e("strong",[r._v("[CVPR 2025] Skip Tuning: Pre-trained Vision-Language Models are Effective and Efficient Adapters Themselves")])]),r._v(" "),e("p",[e("strong",[r._v("Shihan Wu")]),r._v(", Ji Zhang, Pengpeng Zeng, Lianli Gao, Jingkuan Song, Heng Tao Shen")]),r._v(" "),e("p",[r._v("视觉语言模型 · 迁移学习 · 效率")]),r._v(" "),e("p",[r._v("2024.12")]),r._v(" "),e("p",[e("a",{attrs:{href:"https://arxiv.org/pdf/2412.11509",target:"_blank",rel:"noopener noreferrer"}},[r._v("[PDF]"),e("OutboundLink")],1),r._v(" "),e("a",{attrs:{href:"https://arxiv.org/abs/2412.11509",target:"_blank",rel:"noopener noreferrer"}},[r._v("[arXiv]"),e("OutboundLink")],1),r._v(" "),e("a",{attrs:{href:"https://github.com/Koorye/SkipTuning",target:"_blank",rel:"noopener noreferrer"}},[r._v("[代码]"),e("OutboundLink")],1)]),r._v(" "),e("p",[e("img",{attrs:{src:"https://img.shields.io/github/stars/Koorye/SkipTuning?style=for-the-badge&labelColor=%23eeeeee",alt:"GitHub Repo stars"}})])]),r._v(" "),e("ProjectCard",{attrs:{image:"/pubs/capt.png",hideBorder:"true"}},[e("p",[e("strong",[r._v("Rethinking Conditional Prompt Tuning for Vision-Language Models")])]),r._v(" "),e("p",[r._v("Ji Zhang, "),e("strong",[r._v("Shihan Wu")]),r._v(", Pengpeng Zeng, Lianli Gao, Jingkuan Song, Heng Tao Shen")]),r._v(" "),e("p",[r._v("视觉语言模型 · 迁移学习 · 提示调优")]),r._v(" "),e("p",[r._v("2024.8")]),r._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/Koorye/CaPT",target:"_blank",rel:"noopener noreferrer"}},[r._v("[代码]"),e("OutboundLink")],1)]),r._v(" "),e("p",[e("img",{attrs:{src:"https://img.shields.io/github/stars/Koorye/CaPT?style=for-the-badge&labelColor=%23eeeeee",alt:"GitHub Repo stars"}})])]),r._v(" "),e("ProjectCard",{attrs:{image:"/pubs/dept.png",hideBorder:"true"}},[e("p",[e("strong",[r._v("[CVPR 2024] DePT: Decoupled Prompt Tuning")])]),r._v(" "),e("p",[r._v("Ji Zhang*, "),e("strong",[r._v("Shihan Wu")]),r._v("*, Lianli Gao, Heng Tao Shen, Jingkuan Song")]),r._v(" "),e("p",[r._v("视觉语言模型 · 迁移学习 · 提示调优")]),r._v(" "),e("p",[r._v("2023.9")]),r._v(" "),e("p",[e("a",{attrs:{href:"https://arxiv.org/pdf/2309.07439",target:"_blank",rel:"noopener noreferrer"}},[r._v("[PDF]"),e("OutboundLink")],1),r._v(" "),e("a",{attrs:{href:"https://arxiv.org/abs/2309.07439",target:"_blank",rel:"noopener noreferrer"}},[r._v("[arXiv]"),e("OutboundLink")],1),r._v(" "),e("a",{attrs:{href:"https://github.com/Koorye/DePT",target:"_blank",rel:"noopener noreferrer"}},[r._v("[代码]"),e("OutboundLink")],1)]),r._v(" "),e("p",[e("img",{attrs:{src:"https://img.shields.io/github/stars/Koorye/DePT?style=for-the-badge&labelColor=%23eeeeee",alt:"GitHub Repo stars"}})])]),r._v(" "),e("h2",{attrs:{id:"获奖与荣誉"}},[r._v("获奖与荣誉")]),r._v(" "),e("p",[e("router-link",{attrs:{to:"/about/"}},[r._v("→ 完整列表")])],1),r._v(" "),e("p",[e("strong",[r._v("优秀研究生")]),r._v(", 电子科技大学, 2025")]),r._v(" "),e("p",[e("strong",[r._v("国家奖学金")]),r._v(", 中华人民共和国教育部, 2024")]),r._v(" "),e("p",[e("strong",[r._v("优秀毕业生")]),r._v(", 电子科技大学, 2023")])],1)}),[],!1,null,null,null);t.default=n.exports},72:function(r,t,e){}}]);